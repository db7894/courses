{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic unit of a neural network is a perceptron--it takes binary inputs $x_1$, $x_2$, and so on and produces one binary output. \n",
    "Each input has a weight, which expresses its relative importance. If the weighted sum of inputs is less than or equal to a certain threshold, the perceptron outputs 0. If the sum is greater, it outputs 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will redefine this using the term $\\textit{bias}$, denoted $b$, which will now serve as the threshold. The bias is a component of the individual perceptron/neuron that we want to examine. \n",
    "<br>\n",
    "If we have a larger bias, the neuron is less likely to output a one. Thus a smaller bias (or smaller threshold by the previous definition) can be thought of as being less \"strict\" about outputting a one. \n",
    "<br>\n",
    "Our equations are now output = 0 if $w \\cdot x + b \\leq 0$ and output = 1 if $w \\cdot x + b > 0 $. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define $\\textit{learning}$ as calcualting how a change in a given weight changes the weight in the overall output. Since a small change in a weight can actually flip / drastically change the output of a perceptron, we instead use a sigmoid neuron for our nodes. \n",
    "<br>\n",
    "The sigmoid neuron similarly takes the weighted sum and bias as $w \\cdot x + b$, but now applies a nonlinearity to it called the sigmoid function. Thus, the output of a sigmoid neuron is between zero and one. \n",
    "<br>\n",
    "We define the sigmoid function as $\\sigma(z) = \\frac{1}{1 + e^{-z}}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate how the weight changes with respect to a given vector of weights $w$, where we denote each weight $w_j$ and a given bias $b$, we need to calculate partial derivatives. In sum, the change in output with respect to change in bias and weight is:\n",
    "<br>\n",
    "$$ \\Delta output = \\sum_j \\frac{\\partial output}{\\partial w_j} \\Delta w_j + \\frac{\\partial output}{\\partial b} \\Delta b $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we actually train on inputs, we need to get a sense of how \"right\" our network is in terms of guessing. To do this, we define a $\\textbf{cost function}$ C. We define it as follows:\n",
    "$$ C(w,b) = \\frac{1}{2n} \\sum_x \\vert \\vert y(x) - a \\vert \\vert ^2 $$\n",
    "Here, $w$ is defined as the collection of all weights in the network, $b$ is all of the biases, and $n$ is the number of training inputs. The important part here is what's inside the sum: this cost function is called the $\\textbf{Quadratic Cost Function}$ because we are taking the norm of $y(x)-a$ and squaring it. \n",
    "<br>\n",
    "To be explicit, $a$ is the $\\textit{actual}$ output value of a given input $x$--e.g. if $x$ is an image that we want to classify as a digit from $0$ to $9$, $a$ is the label of what the digit actually is. $y(x)$ is the guess that our network made, which we want to compare to $a$. \n",
    "<br>\n",
    "It's also of note that $a$ is actually a vector of outputs for all the training inputs $x$, which is why we must sum over all training inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have two inputs (weights and a bias) into something called the Cost function. The lower the cost the better, since that means our neural network is making guesses about number inputs that are closer to the actual thing. \n",
    "<br>\n",
    "So the problem here is that we have a multi-variable function we want to minimize. Sounds like a problem for the gradient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're dealing with a sufficiently complex neural network, we're dealing with a LOT of weights and biases, so let's simplify the problem to a cost function that takes just two inputs, $C(x,y)$. As a function in two variables, we can more easily visualize $C$ in 3-space by imagining a ball rolling down a hill. Imagine $C$ as a valley with a minimum somewhere. That's exactly what we want to find. \n",
    "<br>\n",
    "Our goal is to find the values $x$,$y$ that will yield a minimum $C(x,y)$. It's hard to guess just what these are, but we can always start with a guess! So let's pick random values and computer $\\Delta C$. We'll get $$ \\Delta C = \\frac{\\partial C}{\\partial x} \\Delta x + \\frac{\\partial C}{\\partial y} \\Delta y.$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so what exactly does that mean? Let's play with an example:\n",
    "<br>\n",
    "Let $C(x,y) = 3x + 2y$. We see immediately that $\\frac{\\partial C}{\\partial x} = 3$ and $\\frac{\\partial C}{\\partial y} = 2$. So from the equation above, $$ \\Delta C = 3 \\Delta x + 2 \\Delta y.$$ \n",
    "<br>\n",
    "This is a little easier to interpret. We'll start with $x,y=1$ to get an initial value of $C(1,1) = 3(1) + 2(1) = 5$. Let's consider what happens when we start to change $x$ and $y$, individually and together. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say I want to find out what hapenens when I increase the value of $x$ by $0.1$, so now I have $x = 1.1$. This is pretty trivial to calculate exactly: $C(x,y) = C(1.01,1) = 3(1.1) + 2(1) = 5.3$. But we want to generalize this figure, so we'll compute it with derivatives. \n",
    "<br>\n",
    "In this case, $\\Delta x = 0.1$, and $\\Delta y = 0$ since we chose to keep $y$ constant. As before, $\\frac{\\partial C}{\\partial x} = 3$ and $\\frac{\\partial C}{\\partial y} = 2$. So we calculate $\\Delta C$ as follows: $$\\Delta C = 3\\Delta x + 2\\Delta y = 3(0.1) + 2(0) = 0.3.$$\n",
    "So when we increase $x$ by $0.1$, our cost $C$ increases by $0.3$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's formalize this in terms of gradients. The gradient of a function is defined as a vector of its partial derivatives with respect to each of its inputs. In this case, $ \\nabla C =  \\left ( \\dfrac{\\partial C}{\\partial x}, \\dfrac{\\partial C}{\\partial y}  \\right ) ^T $, where we've introdced the transpose to make $C$ a column vector.\n",
    "<br>\n",
    "We'll similarly re-define the inputs to $C$ as a vector $x = (x,y)^T$. This allows us to re-write the changes in $x$ and $y$ as a vector $\\Delta x = \\left ( \\Delta x, \\Delta y \\right )^T$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'll re-write $\\Delta C$ in terms of a dot product of $ \\nabla C $ and $ \\Delta x $. Note that the dot product is defined for two column vectors, and is an element-wise sum of their members. \n",
    "<br>\n",
    "So we have $$ \\Delta C = \\nabla C \\cdot \\Delta x $$ \n",
    "or $$ \\Delta C = \\left ( \\dfrac{\\partial C}{\\partial x}, \\dfrac{\\partial C}{\\partial y}  \\right ) ^T \\cdot \\left ( \\Delta x, \\Delta y \\right )^T = \\frac{\\partial C}{\\partial x} \\Delta x + \\frac{\\partial C}{\\partial y} \\Delta y $$ as before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a little more intuition about how to minimize $C$. We can figure out how $C$ changes when we increase $x$ or $y$ by a small amount. If $C$ increases when we change $x$ by a small amount, then we can iteratively decrease $x$ by that small amount to achieve a smaller $C$. We do the same with the other inputs to $C$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we'll define the update rule. We'll call the greek letter eta, written $\\eta$, our \"learning rate\". This tells us how fast we'll change the inputs to $C$ as we attempt to approach its minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to update our parameters, so we'll compute the formula we wrote above, $ \\Delta C = \\nabla C \\cdot \\Delta x $. We want to change $x$ in such a way as to make $\\Delta C$ negative, so we'll do the following: \n",
    "<br>\n",
    "Choose $\\Delta x = - \\eta \\nabla C$. \n",
    "<br>\n",
    "Update $x$ by that amount. If our initial value was $x_{initial}$, our new value is $x_{updated} = x_{initial} - \\eta \\nabla C$.\n",
    "<br>\n",
    "If we go back to the formula for $\\Delta C$, we can see that this will actually produce a negative change in our cost function:\n",
    "$$ \\Delta C = \\nabla C \\cdot \\Delta x = - \\eta \\nabla C \\cdot \\nabla C = - \\eta \\vert \\vert \\nabla C \\vert \\vert ^2, $$\n",
    "where $ \\vert \\vert \\nabla C \\vert \\vert ^2 $ is the norm squared of $\\nabla C $ since the gradient of $C$ is a vector. This is essentially equivalent to the magnitude, so given the way we defined $\\nabla C$, \n",
    "$$ \\vert \\vert \\nabla C \\vert \\vert = \\sqrt{ \\left ( \\dfrac{\\partial C}{\\partial x} \\right )^2 + \\left ( \\dfrac{\\partial C}{\\partial y} \\right )^2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the Trivial Inequailty (my favorite inequality), we know $ \\vert \\vert \\nabla C \\vert \\vert ^2 \\geq 0 $, so our update $ - \\eta \\vert \\vert \\nabla C \\vert \\vert ^2 < 0 $. Thus, $$ \\Delta C < 0, $$\n",
    "and our choice of update actually decreases the cost $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Nota bene}$: If we choose too large a learning rate $\\eta$, we can actually overshoot the minimum of $C$ we want. Imagine $C(x,y) = x^2$. We can see that the minimum value for $C$ is $0$. Let's say I begin with choosing a value of $5$ for $x$. Then I know $ \\dfrac {\\partial C}{\\partial x} = 2x $, so $\\nabla C = 2x $. I want to iteratively decrease $x$ so that I reach the minimum value of $C$, which is $0$. Let's say I choose a really high learning rate $\\eta = 40$. Then when I update $x$, I will change it in this way:\n",
    "$$ x_{updated} = x_{original} - \\eta \\nabla C = -40 \\cdot 2x. $$\n",
    "Since $x = 5$, our new value of $x$ is:\n",
    "$$ x_{updated} = 5 - 40 \\cdot 2(5) = 5 - 400 = -395. $$\n",
    "This is a really drastic example, but it illustrates how we completely overshot finding the minimum value of $C$ by choosing an unreasonably high learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Application: Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take what we just learned and apply it to what would happen in an actual neural network! We've got a bunch of weights (from a neuron in a given layer to a neuron in the next layer) and biases (which each neuron has). \n",
    "<br>\n",
    "Imagine we're looking at one neuron in a neural network. Our goal is to figure out how we can update the following parameters to produce a lower cost:\n",
    "<br>\n",
    "1. A specific weight from a neuron in the previous layer, which we will denote $w_k$.\n",
    "<br>\n",
    "2. The bias of this neuron we want to examine, which we'll call $b_l$.\n",
    "\n",
    "As before, we'll compute the partial derivatives $ \\dfrac{\\partial C}{\\partial w_k} $ and $ \\dfrac{\\partial C}{\\partial b_l} $ to see how the cost changes when we change each of $w_k$ and $b_l$ by a small amount. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same update rules, we will update the weight and bias as follows:\n",
    "$$ w_k \\rightarrow w_k' = w_k - \\eta \\dfrac{\\partial C}{\\partial w_k}, $$\n",
    "$$ b_l \\rightarrow b_l' = b_l - \\eta \\dfrac{\\partial C}{\\partial b_l}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference? In $\\textbf{stochastic gradient descent}$, we'll learn--find partials and update weights and biases, in \"mini-batches\". If we have a set of training data, we'll randomly select (this is why we call it $\\textit{stochastic}$) groups of training inputs $X_1, X_2, \\cdots X_m$, where we've chosen a large enough $m$ that we can generalize the results obtained from the training data. That is, if we were to choose any mini-batch of size $m$ from the training samples, we can expect the average value of $\\nabla C_x$ to be roughly the same across all of them. \n",
    "<br>\n",
    "At this point, we've also reformulated the cost to a for $ C = \\dfrac{1}{n} \\sum_x C_x $, where $ C_x = \\dfrac{ \\vert \\vert y(x) -a \\vert \\vert ^2}{2} $ for some input $x$, and we're computing this loss over $n$ examples $C_x$. \n",
    "<br>\n",
    "So the strategy is to find the gradients of mini-batches, and average those gradients over all the mini-batches. In not words,\n",
    "$$ \\frac{ \\sum_{j=1}^{m} \\nabla C_{X_j} }{m} \\approx \\frac{ \\sum_x \\nabla C_x}{n} = \\nabla C $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So to re-iterate our update rules, we'll pick a randomly-chosen mini-batch of training inputs, and with these update $w_k$ and $b_l$ as follows:\n",
    "$$ w_k \\rightarrow w_k' = w_k - \\dfrac{\\eta}{m} \\sum_j \\dfrac{\\partial C_{X_j}}{\\partial w_k}, $$\n",
    "$$ b_l \\rightarrow b_l' = b_l - \\dfrac{\\eta}{m} \\sum_j \\dfrac{\\partial C_{X_j}}{\\partial b_l} $$\n",
    "where we sum over all training examples $X_j$ in the current mini-batch. \n",
    "<br>\n",
    "In an $\\textit{epoch}$, we keep picking mini-batches until we've exhausted the training inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation: Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the following definitions (from Michael Nielsen's Neural Networks and Deep Learning a.k.a:\". a better version of these notes):\n",
    "<br>\n",
    "$ w_{jk}^l $ denotes the weight for the connection from the $k^{th}$ neuron in the $(l-1)^{th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer.\n",
    "<br> \n",
    "$b_j^l$ denotes the bias of the $j^{th}$ neuron in the $l^{th}$ layer.\n",
    "<br>\n",
    "$a_j^l$ denotes the activation of the $j^{th}$ neuron in the $l^{th}$ layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we'll define the activation of a neuron as a non-linearity applied to the weighted sum of its input activations, plus the bias:\n",
    "$$ a_j^l = \\sigma \\left ( \\sum_k w_{jk}^l a_k^l + b_j^l \\right ), $$\n",
    "where we sum over all neurons $k$ in the $ (l-1)^{th} $ layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can re-write the above equation in a vectorized form:\n",
    "<br>\n",
    "$w^l$ denotes the weights connected to the $l^{th}$ layer of neurons. More specifically, $w^l$ is the collection of weights $w_{jk}^l$ from the $k^{th}$ neuron in the $(l-1)^{th}$ layer to the $j^{th}$ neuron in the $l^{th}$ layer--this collects $\\textbf{all}$ weights from neurons in the $(l-1)^{th}$ layer to neurons in the $l^{th}$ layer.\n",
    "<br>\n",
    "$b^l$ is the vector of biases with components $b_j^l$. It is a collection of the biases of all neurons in the $l^{th}$ layer.\n",
    "<br>\n",
    "Similarly, $a^l$ is a collection of all activations $a_j^l$ of neurons in the $l^{th}$ layer.\n",
    "<br>\n",
    "Furthermore, the sigmoid function has its own property. If we have a vector $v$ and take the sigmoid, we will have a vector of elements, each of which is sigmoid applied to an element $v_j$ of $v$, which we can dentoe $ \\sigma(v)_j $. This is equal to $ \\sigma(v_j)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can vectorize the above equation:\n",
    "$$ a^l = \\sigma \\left ( w^l a^{l-1} + b^l \\right ) = \\sigma (z^l), $$\n",
    "where $ z^l = w^l a^{l-1} + b^l $ is the vector of $\\textit{weighted inputs}$ to the neurons in layer $l$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's recall and reiterate a few things:\n",
    "<br>\n",
    "First, we'll reive the cost function:\n",
    "$$ C = \\frac{1}{2n} \\sum_x \\vert \\vert y(x) - a^L(x) \\vert \\vert ^2, $$\n",
    "where $n$ is the number of training examples,\n",
    "<br>\n",
    "$y = y(x) $ is the vector of desired outputs,\n",
    "<br>\n",
    "$L$ is the number of layers in the network,\n",
    "<br>\n",
    "$a^L(x)$ is the vector of activations output from the network's final layer when $x$ is an input--in other words, the $\\textbf{actual result}$ the network gave us.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A few assumptions:\n",
    "<br>\n",
    "We can write $C$ as an average over training examples $$ C = \\frac{1}{n} \\sum_x C_x, $$ where the cost for a single training example is $$ C_x = \\frac{1}{2} \\vert \\vert y - a^L \\vert \\vert ^2. $$\n",
    "<br>\n",
    "We will refer to a training example now using $C = C_x$.\n",
    "<br>\n",
    "$C$ is a function of $a^L$ and not $y$, since $y$ is a desired output and therefore fixed with regard to $x$. So we can re-write things (once again) as $$ C = \\frac{1}{2} \\vert \\vert y - a^L \\vert \\vert ^2 = \\frac{1}{2} \\sum_j \\left ( y_j - a_j^L \\right )^2, $$\n",
    "where the latter part of the equation arises from converting the norm (squared) of a vector to the sum of components. \n",
    "<br>\n",
    "To clarify further, if our outputs from the network were $ y = (2,3,1) $ and the desired values were $ (1,3,0) $, we would first calculate $$ \\vert \\vert y - a^L \\vert \\vert ^2 = \\vert \\vert (2-1,3-3,1-0) \\vert \\vert ^2 $$ \n",
    "$$ = \\left ( \\sqrt{(2-1)^2 + (3-3)^2 + (1-0)^2} \\right )^2 = 2, $$\n",
    "which is the same as if we had taken the squares of pairwise differences of elements from $y$ and $a^L$ and summed them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also define the $\\textbf{Hadamard Product}$. If we let $s$ and $t$ be two vectors, then we define the hadamard product as the vector resulting from elementwise multiplication of the members of $s$ and $t$. So we write the $j^{th}$ element of this vector as $ (s \\odot t)_j = s_j t_j $."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll define the error of the $j^{th}$ neuron in the $l^{th}$ layer. \n",
    "<br>\n",
    "Since the weighted input $z$ is a function of weights and biases, it should make sense that we can re-write the cost $C$, itself a function of weights and biases, as a function $C(z)$ of the weighted input. So, if we change the weighted input $ z_j^l $ to the $j^{th}$ neuron in the $l^{th}$ layer by a small amount $ \\Delta z_j^l $, then the cost will change proportionally to its partial derivative w/r/t that weighted input. In other words,\n",
    "$$ \\Delta C = \\dfrac{\\partial C}{\\partial z_j^l} \\Delta z_j^l.$$ \n",
    "We define the $\\textbf{error}$ of the $j^{th}$ neuron in the $l^{th}$ layer as $\\delta_j^l = \\dfrac{\\partial C}{\\partial z_j^l}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation: The 4 Equations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. An equation for the error $\\delta^L$ in the output layer. The components are given by:\n",
    "$$ \\delta_j^L = \\dfrac{\\partial C}{\\partial c_j^L}\\sigma'(z_j^L) $$\n",
    "where $ \\delta_j^L $ is the error of the $j^{th}$ neuron in the output layer, $\\dfrac{\\partial C}{\\partial c_j^L}$ is the partial derivative of the cost with respect to the activation of that same neuron, and $z_j^L$ is the weighted input to that neuron. \n",
    "<br>\n",
    "We can also write the vector of errors of $\\textit{all}$ neurons in the output layer as:\n",
    "$$ \\delta^L = \\nabla_a C \\odot \\sigma'(z^L), $$\n",
    "where $\\nabla_a C$ is the vector of partial derivatives $ \\dfrac{\\partial C}{\\partial a_j^L} $, or the rate of change of $C$ w/r/t/ the output activations of the network. The Hadamard Product here denotes taking the elements of the vectors $ \\dfrac{\\partial C}{\\partial a_j^L} $ and $\\sigma;(z^L)$ and multiplying them elementwise to produce an output vector (that is the same size as both vector involved in the product, which should be equal to the number of neurons in the output layer). \n",
    "<br>\n",
    "Additionally, $\\sigma'(z_j^L)$ tells us how fast the activation function is changing at the location of $z_j^L$. \n",
    "<br>\n",
    "<br>\n",
    "\n",
    "2. An equation for the error $\\delta^l$ in terms of the error in the next layer, $\\delta^{l+1}$. \n",
    "$$ \\delta^l = \\left ( (w^{l+1})^T \\delta^{l+1} \\right ) \\odot \\sigma' (z^l), $$\n",
    "where $ (w^{l+1})^T $ is the transpose of the vector of weights in the $ (l+1)^{th} $ layer. \n",
    "<br>\n",
    "So if we know the error in the $(l+1)^{th}$ layer $\\delta^{l+1}$, we can apply the vector of activations $ (w^{l+1})^T $ to it to propagate the error backward through the network, computing the error in the previous layer. \n",
    "<br>\n",
    "When we take the Hadamard Product $\\odot \\sigma'(z^l)$, we move back through the activation function in the $l^{th}$ layer to get the error in the weighted input to layer $l$. \n",
    "<br>\n",
    "<br>\n",
    "3. An equation for the rate of change of the cost w/r/t/ any bias in the network:\n",
    "$$ \\dfrac{\\partial C}{\\partial b_j^l} = \\delta_j^l, $$\n",
    "or in other words, the error of the $j^{th}$ neuron in the $l^{th}$ layer is exactly equal to the rate of change of the cost with respect to the bias of that neuron. \n",
    "<br>\n",
    "<br>\n",
    "4. An equation for the rate of change of the cost w/r/t/ any weights in the network:\n",
    "$$ \\dfrac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l $$\n",
    "or $ \\dfrac{\\partial C}{\\partial w} = a_{in} \\delta_{out} $, where $a_{in}$ is the activation that the neuron inputs to the weight $w$, and $\\delta_{out}$ is the error of the neuron output from the weight $w$. So if we imagine two neurons connected by a line with a weight, the in neuron is the one in an earlier layer, and the out neuron is the one in the following layer that the first neuron is connected to. \n",
    "<br>\n",
    "If the input activation $a_{in}$ is small, then $\\dfrac{\\partial C}{\\partial w}$ is small, which causes the weight to learn slowly--it won't change much during gradient descent because the cost will change only very slightly as the weight does, meaning the update will cause the weight to change very little each iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Backpropagation Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Input $x$: and set the corresponding activation $a^1$ for the input layer.\n",
    "<br>\n",
    "2. Feedforward: For each $ l = 2,3, \\cdots , L$, compute the vector of weighted inputs for the layer $ z^l = w^l a^{l-1} + b^l $, and the vector of activations for the layer $ a^l = \\sigma (z^l) $. \n",
    "<br>\n",
    "3. Output error $\\delta^L$: Compute the error from the first backpropagation equation $ \\delta^L = \\nabla_a C \\odot \\sigma' (z^L) $. \n",
    "<br>\n",
    "4. Backpropagate the error: For each layer (besides the first and the last), starting from $ l = L-1, L-2, \\cdots , 2 $, compute \n",
    "$$ \\delta^l = \\left ( (w^{l+1})^T \\delta^{l+1} \\right ) \\odot \\sigma; (z^l). $$\n",
    "<br>\n",
    "5. Calculate the output: The gradient of the cost function with respect to specific weights and biases is given by $$ \\dfrac{\\partial C}{\\partial w_{jk}^l} = a_k^{l-1} \\delta_j^l , $$\n",
    "$$ \\dfrac{\\partial C}{\\partial b_j^l} = \\delta_j^l. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to apply SGD to learn with a mini-batch, we can do the following:\n",
    "<br>\n",
    "1. Input a set of training examples.\n",
    "2. For each training example $x$, set the corresponding activation $a^{x,1}$, the vector of activations in the first layer of the network when we input x, and perform the following:\n",
    "<br>\n",
    "$\\textbf{Feedforward}$: For each layer $ l = 2, 3, \\cdots , L $, compute the vector of weighted inputs to the layer $ z^{x,l} = w^l a^{x,l-1} + b^l $ and the vector of activations of neurons in the $l^{th}$ layer $a^{x,l} = \\sigma(z^{x,l}$. \n",
    "<br>\n",
    "$\\textbf{Output error} \\ \\delta^{x,L} $: Compute the vector of erros in the output layer $ \\delta^{x,L} = \\nabla_a C_x \\odot \\sigma'(z^{x,L}) $.\n",
    "<br>\n",
    "$\\textbf{Backpropagate the error}$: Starting from the second to last layer, for each $ l = L-1, L-2, \\cdots 2 $, compute the vector of errors of neurons in the $l^{th}$ layer $$ \\delta^{x,l} = \\left ( (w^{l+1})^T \\delta^{x,l+1} \\right ) \\odot \\sigma' (x^{x,l}). $$\n",
    "<br>\n",
    "3. Perform the Gradient Descent: Starting from the output layer, for each $ l = L, L-1, \\cdots , 2 $, update the vector of weights in layer $l$ according to the rule\n",
    "$$ w^l \\rightarrow w^l - \\dfrac{\\eta}{m} \\sum_x \\delta^{x,l} (a^{x,l-1})^T $$\n",
    "and vector of biases in layer $l$ according to the update rule\n",
    "$$ b^l \\rightarrow b^l - \\dfrac{\\eta}{m} \\sum_x \\delta^{x,l}. $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Code for Backpropagation (from Michael Nielsen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConvNets: Types of Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Convolutional Neural Network is somewhat different from a normal neural network--it makes the explicit assumption that the inputs are images, which lets us give it certain properties. Unlike a normal neural network, the layers of a ConvNet have width, height, and depth--you can think of a Convolutional Layer as a series of \"filters\", each a matrix that gets applied to the input or a part of the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Convolutional Layer}$: \n",
    "A convolutional layer is especially useful when dealing with images, since we can use its filters to discover different \"features\" of the image--e.g. figure out if it contains edges in a certain location, and so on. Recall that an image can be expressed as a matrix of values. We'll look at an example. We'll also pretend we're working with a 5x5 greyscale image, where each pixel is represented by a numerical value between $0$ and $255$. \n",
    "\n",
    "   \\begin{bmatrix}\n",
    "     11 & 77 & 0 & 150 & 30 \\\\\n",
    "     32 & 200 & 3 & 102 & 17 \\\\\n",
    "     12 & 67 & 33 & 1 & 42 \\\\\n",
    "     1 & 70 & 222 & 13 & 0 \\\\ \n",
    "     3 & 14 & 0 & 8 & 1\n",
    "   \\end{bmatrix}\n",
    "\n",
    "Now we'll define the $\\textit{convolution}$ operation. We'll take a 3x3 matrix, called a \"filter\" in our convolutional layer:\n",
    "\n",
    "   \\begin{bmatrix}\n",
    "     1 & 0 & 1 \\\\\n",
    "     0 & 1 & 0 \\\\\n",
    "     1 & 0 & 1 \\\\\n",
    "   \\end{bmatrix}\n",
    "and apply it in the following way:\n",
    "<br>\n",
    "We'll take the 3x3 matrix and \"slide\" it across all 3x3 sections of our original image matrix. We'll then take the sum of elementwise products of the elements of the two matrices. This gives us the desired \"feature\" of this section of the matrix. For example, let's convolve our filter with the upper-left 3x3 section of the image matrix: \n",
    "   \\begin{bmatrix}\n",
    "     11 & 77 & 0 \\\\\n",
    "     32 & 200 & 3 \\\\\n",
    "     12 & 67 & 33\n",
    "   \\end{bmatrix}\n",
    "We'll multiply it elementwise with our filter and take the sum, getting\n",
    "$$ feature = 1(11) + 0(77) + 1(0) + 0(32) + 1(200) + 0(3) + 1(12) + 0(67) + 1(33) = 256 $$\n",
    "We'll also use a $\\textbf{stride}$ of $1$, meaning that when we slide our 3x3 matrix along the original matrix, we'll move it one pixel at a time, instead of 2, or 3, or more. If we compute the convolution of our filter with the input image, we'll get the following feature matrix:\n",
    "   \\begin{bmatrix}\n",
    "     256 & 298 & 207 \\\\\n",
    "     325 & 418 & 243 \\\\\n",
    "     118 & 312 & 89\n",
    "   \\end{bmatrix}\n",
    "\n",
    "You'll notice that the matrix of features we computed is smaller than the original matrix--in fact, we lost the entire one-pixel border of the image! But fear not, we can fix this with the $\\textbf{zero-padding layer}$. This adds a border of zeros around the edge of the image. If we apply this to the matrix, we'll end up with a 7x7 image that has zeros around the border, and we won't lose any information after applying the filter to the entire image. \n",
    "<br>\n",
    "We've only applied one filter here, but the convolution layer of a network has a  $\\textbf{depth}$, which means that we would apply many of these filters to the image in the layer. \n",
    "<br>\n",
    "Furthermore, unlike in a regular neural network, since we've got a much larger architecture it's impractical for every neuron to be connected to every neuron in a previous layer. So we only connect each neuron to a local region of the input layer--the spatial extent of this connection is called hte $\\textit{receptive field}$ of the neuron. \n",
    "<br>\n",
    "Another logistical point is $\\textit{parameter sharing}$: We generally want to control the number of parameters in a network to avoid overfitting. If we take a slice of a convolutional layer in our network we get a filter: We'll use the same input weights and same bias for each neuron in that slice, since we make the assumption that if the weight and bias we are using was useful to compute somewhere, it's probably useful to compute somewhere else. \n",
    "<br>\n",
    "We can see this when looking at the image, but we can also compute the size of the output volume (the feature matrix after applying a filter) as follows:\n",
    "$$ size = \\dfrac{W - F + 2P}{S} + 1, $$\n",
    "where $W$ is the input volume size (e.g. our 25 for a 5x5 matrix), $F$ is the field size of the convolutional layer's neurons, $S$ is the stride, and $P$ is the amount of zero-padding (how many pixels thick the layer of zeros on the border is)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{The Pooling Layer}$: This layer progressively reduces the spatial size of the matrix representation of our image--as a result, we can reduce the number of parameters computed in the network so that we aviod overfitting. We often do this with an operation called $\\textbf{max pooling}$. Imagine we're looking at a 2x2 section of our image (at an arbitrary point in the network, so we might just be looking at features):\n",
    "   \\begin{bmatrix}\n",
    "     1 & 0 \\\\\n",
    "     0 & 77\n",
    "   \\end{bmatrix}\n",
    "This example is a bit contrived, but it illustrates the point that one of these features is clearly more \"important\" than the rest, in the sense that it has a much higher value. The 2x2 max pooling operation reduces this 2x2 matrix of features to a $\\textbf{single}$ value: $77$. \n",
    "<br>\n",
    "As you can probably imagine by this point, the amount of computation that goes on in a CNN is incredibly computationally expensive--in order to test faster and avoid the risk of overfitting from having too many parameters, it's not a bad idea to try to reduce the number of parameters as we move further in the network. If we've applied a few convolutional layers to an image matrix to get some different features, chances are if we $\\textbf{downsample}$, or reduce the resolution of our matrix, we won't lose much imporant information. \n",
    "<br>\n",
    "Another layer of note is the $\\textbf{Fully-Connected Layer}$, where each neuron in a layer is fully connected to all neurons in the previous layer. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
