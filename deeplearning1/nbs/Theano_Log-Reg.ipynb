{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 5103 on context None\n",
      "Preallocating 10867/11439 Mb (0.950000) on cuda\n",
      "Mapped name None to device cuda: Tesla K40c (0000:81:00.0)\n"
     ]
    }
   ],
   "source": [
    "import six.moves.cPickle as pickle\n",
    "import gzip\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "from theano import *\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to implement a probabilistic, linear classifier parameterized by a weight matrix $W$ and a bias vector $b$. \\newline\n",
    "The probability that an input vector $x$ is a member of a class $i$, which we will call the value of a stochastic variable $Y$, is\n",
    "$$ P(Y = i \\vert x,W,b) = softmax(Wx + b) $$\n",
    "$$ = \\frac{e^{W_ix + b_i}}{\\sum_j e^{W_j+b_j}} $$\n",
    "\n",
    "The model's prediction $y_{pred}$ is the class that has the highest probability, or:\n",
    "$$ y_{pred} = argmax_iP(Y = i \\vert x,W,b) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a likelihood $\\mathcal{L}$ and loss $l$:\n",
    "$$ \\mathcal{L}(\\theta = \\{ W,b \\}, \\mathcal{D}) = \\sum_{i=0}^{\\vert \\mathcal{D} \\vert} \\log(P(Y = y^{(i)} \\vert x^{(i)},W,b)) $$\n",
    "$$ l(\\theta = \\{ W,b \\},\\mathcal{D}) = -\\mathcal{L}(\\theta = \\{ W,b \\}, \\mathcal{D}) $$\n",
    "We will use the SGD algorithm w/ mini-batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogisticRegression(object):\n",
    "    def __init__(self, input, n_in, n_out):\n",
    "        \"\"\"\n",
    "        initialize parameters:\n",
    "        input: theano.tensor.TensorType\n",
    "        input param: symbolic var describing input (1 minibatch)\n",
    "        \n",
    "        type n_in: int\n",
    "        param n_in: # of input units, dimension of space of datapoints\n",
    "        \n",
    "        type n_out: int\n",
    "        param n_out: # of output units\n",
    "        \"\"\"\n",
    "        \n",
    "        #initialize weights W as a matrix of shape (n_in, n_out) w/ 0s\n",
    "        self.W = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_in,n_out),\n",
    "                dtype = theano.config.floatX\n",
    "            ),\n",
    "            name='W',\n",
    "            borrow=True\n",
    "        )\n",
    "        \n",
    "        #initialize biases as vector of n_out zeros\n",
    "        self.b = theano.shared(\n",
    "            value=numpy.zeros(\n",
    "                (n_out,),\n",
    "                dtype=theano.config.floatX\n",
    "            ),\n",
    "            name='b',\n",
    "            borrow=True\n",
    "        )\n",
    "        \n",
    "        \n",
    "        #now we will compute matrix of class-membership probabilities where\n",
    "        # W is a mtrix where column-k is separation hyperplane for class k\n",
    "        # x is a matrix where row-j is input trainign sample j\n",
    "        # b is a vector where element k is free parameter of hyperplane k\n",
    "        self.p_y_given_x = T.nnet.softmax(T.dot(input,self.W + self.b))\n",
    "        \n",
    "        # the actual prediction is the class w/ greatest probability\n",
    "        self.y_pred = T.argmax(self.p_y_given_x, axis=1)\n",
    "        \n",
    "        # model parameters\n",
    "        self.params = [self.W,self.b]\n",
    "        \n",
    "        #keep track of input\n",
    "        self.input = input\n",
    "        \n",
    "    def negative_log_likelihood(self,y):\n",
    "        \"\"\"\n",
    "        Return mean of the negative log likelihood of prediction of this model under given target distribution\n",
    "        \"\"\"\n",
    "        \n",
    "        # y.shape[0] is the number of rows in y, e.g. number of examples in minibatch\n",
    "        # T.arange(y.shape[0]) is a vector which is a matrix of log probabilities\n",
    "        return -T.mean(T.log(self.p_y_given_x)[T.arange(y.shape[0]), y])\n",
    "    \n",
    "    def errors(self,y):\n",
    "        \"\"\"\n",
    "        return float w/ num of errors in minibatch over total # examples in minibatch\n",
    "        \"\"\"\n",
    "        \n",
    "        #check y vs y_pred dimensions\n",
    "        if y.ndim != self.y_pred.ndim:\n",
    "            raise TypeError(\n",
    "                'y should have the same shape as self.y_pred',\n",
    "                ('y',y.type,'y_pred',self.y_pred.type)\n",
    "            )\n",
    "            \n",
    "        #check y is correct type\n",
    "        if y.dtype.startswith('int'):\n",
    "            return T.mean(T.neq(self.y_pred,y))\n",
    "        else:\n",
    "            raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(dataset):\n",
    "    data_dir, data_file = os.path.split(dataset)\n",
    "    if data_dir == \"\" and not os.path.isfile(dataset):\n",
    "        new_path = os.path.join(\n",
    "            os.path.split(__file__)[0],\n",
    "            \"..\",\n",
    "            \"data\",\n",
    "            dataset\n",
    "        )\n",
    "        if os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "            from six.moves import urllib\n",
    "            origin = (\n",
    "            'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "            )\n",
    "            print('Downloading data from %s' % origin)\n",
    "            urllib.request.urlretrieve(origin, dataset)\n",
    "        \n",
    "        print('... loading data')\n",
    "        \n",
    "        #load dataset\n",
    "        with gzip.open(dataset,'rb') as f:\n",
    "            try:\n",
    "                train_set, valid_set, test_set = pickle.load(f, encoding='latin1')\n",
    "            except:\n",
    "                train_set,valid_set,test_set = pickle.load(f)\n",
    "                \n",
    "        def shared_dataset(data_xy, borrow=True):\n",
    "            #load dataset into shared vars.\n",
    "            data_x,data_y = data_xy\n",
    "            shared_x = theano.shared(numpy.asarray(data_x,\n",
    "                                                dtype=theano.config.floatX),\n",
    "                                   borrow=borrow)\n",
    "            shared_y = theano.shared(numpy.asarray(data_y,\n",
    "                                                   dtype=theano.config.floatY),\n",
    "                                     borrow=borrow)\n",
    "            return shared_x, T.cast(shared_y, 'int32')\n",
    "        \n",
    "        test_set_x, test_set_y = shared_dataset(test_set)\n",
    "        valid_set_x, valid_set_y = shared_dataset(valid_set)\n",
    "        train_set_x, train_set_y = shared_dataset(train_set)\n",
    "        \n",
    "        rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\n",
    "               (test_set_x, test_set_y)]\n",
    "        return rval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we instantiate the class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = T.matrix('x')\n",
    "y = T.ivector('y') # labels as 1D vector of [int] labels\n",
    "\n",
    "# cosntruct log reg class\n",
    "# each MNIST image has size 28x28\n",
    "classifier = LogisticRegression(input=x, n_in = 28*28, n_out = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define a cost var to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cost = classifier.negative_log_likelihood(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we learn the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "g_W = T.grad(cost=cost, wrt=classifier.W)\n",
    "g_b = T.grad(cost=cost, wrt=classifier.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we'll write a function to perform one step of gradient descent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_set_x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-5f3acc129328>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mupdates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mupdates\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     givens={\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_set_x\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_set_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_set_x' is not defined"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "index = T.iscalar('index')\n",
    "\n",
    "updates = [(classifier.W, classifier.W - learning_rate * g_W),\n",
    "               (classifier.b, classifier.b - learning_rate * g_b)]\n",
    "\n",
    "# compiling a Theano function `train_model` that returns the cost, but in\n",
    "# the same time updates the parameter of the model based on the rules\n",
    "# defined in `updates`\n",
    "train_model = theano.function(\n",
    "    inputs=[index],\n",
    "    outputs=cost,\n",
    "    updates=updates,\n",
    "    givens={\n",
    "        x: train_set_x[index * batch_size: (index + 1) * batch_size],\n",
    "        y: train_set_y[index * batch_size: (index + 1) * batch_size]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
